# README

## **Author**

Ümit Altar Binici

## **Directory Structure**

```plaintext
|  
├── main.py               # Entry point for training and evaluating the POS tagger on the full dataset.  
├── demo.py               # Demonstration script for training and evaluating on a smaller dataset.  
├── preprocessor.py       # Preprocesses the dataset by tokenizing inputs and aligning POS labels.  
├── dataloader.py         # Prepares data loaders for training, validation, and testing.  
├── trainer.py            # Handles model training and validation.  
├── evaluator.py          # Evaluates the model on the test set.  
├── model.py              # Defines the POS tagging model based on XLM-RoBERTa.  
├── tokenized_data.pth    # Saved preprocessed dataset (generated by `preprocessor.py`).  
├── demo_results.txt      # Results of the demonstration run on a smaller dataset.  
├── README.md             # This README file.  
```

## **Versions**

- **Python:** 3.11.5  
- **Libraries:**  
  - `transformers`: 4.34.0  
  - `datasets`: 2.17.1  
  - `torch`: 2.0.1  

## **Runtime**

Approximate runtimes for each script on a typical machine:

1. **`main.py`:**  
   - **Runtime:** ~10–20 minutes (depends on dataset size and hardware).  
   - Trains the model on the full dataset and evaluates on the test set.

2. **`demo.py`:**  
   - **Runtime:** ~1–2 minutes.  
   - Demonstrates training and evaluation on a smaller dataset.

## **Additional Features**

- **Preprocessing (`preprocessor.py`)**  
   - **Feature:** Tokenizes sentences and aligns POS labels with tokenized words.  
   - **Implementation:** Uses `XLM-RoBERTaTokenizerFast` for efficient tokenization.

- **Data Loading (`dataloader.py`)**  
   - **Feature:** Creates PyTorch DataLoaders for efficient batch processing.  
   - **Implementation:** Supports training, validation, and testing splits.

- **Model Architecture (`model.py`)**  
   - **Feature:** XLM-RoBERTa-based model with a custom classification head for POS tagging.  
   - **Implementation:** Freezes embeddings and encoder layers for efficiency.

- **Flexible Demonstration (`demo.py`)**  
   - **Feature:** Trains and evaluates the model on a small subset of the dataset for debugging.  
   - **Output:** Results saved to `demo_results.txt`.

## **Usage**

1. **Run Full Training and Evaluation (`main.py`):** 

```bash
   python main.py
```

   Outputs:
   - Trained model saved as `pos_model.pth`.
   - Evaluation results displayed in the console.

2. **Run Demonstration (`demo.py`):**

```bash
   python demo.py
```

   Outputs:
   - Results of training and evaluation on a small dataset saved to `demo_results.txt`.

3. **Preprocess the Dataset (`preprocessor.py`):**

```bash
   python preprocessor.py
```

   Outputs:
   - Preprocessed dataset saved as `tokenized_data.pth`.

4. **Run Unit Evaluation (`evaluator.py`):**

```bash
   python evaluator.py
```

   Outputs:
   - Accuracy and loss metrics for the test set.

## **Learning Outcomes**

This project demonstrates:
- End-to-end pipeline for POS tagging using a Transformer-based model.
- Effective preprocessing for tokenization and label alignment.
- Model training and evaluation using PyTorch and Hugging Face libraries.
- Debugging and quick testing with a flexible demonstration script.
